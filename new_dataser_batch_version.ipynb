{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERset(Dataset):\n",
    "    def __init__(self,mode):\n",
    "        with open(\"textData.pkl\", \"rb\") as f:\n",
    "            self.data = pickle.load(f)\n",
    "            self.train_or_test = mode\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.data[index]['name']\n",
    "        char_tokens_tensors = self.data[index]['char_input']\n",
    "        word_tokens_tensors = self.data[index]['word_input']\n",
    "        if self.train_or_test=='train':\n",
    "            label_ids = self.data[index]['char_tag']\n",
    "        else:\n",
    "            label_ids = None\n",
    "        find_sep = np.argwhere(char_tokens_tensors==3)\n",
    "        \n",
    "        thesis_or_context=find_sep[0][0].item()+1\n",
    "        segments_tensor = torch.tensor([0]*thesis_or_context +[1]*(char_tokens_tensors.shape[0]-thesis_or_context))\n",
    "        \n",
    "        context_len= char_tokens_tensors.shape[0]\n",
    "        if context_len >=512:\n",
    "            sep_tensor = torch.tensor([3])\n",
    "            other_label = torch.tensor([1]*1+[0]*20,dtype=torch.float)\n",
    "            other_label = torch.unsqueeze(other_label,0)\n",
    "            char_tokens_tensors = char_tokens_tensors[0:511]\n",
    "            word_tokens_tensors = word_tokens_tensors[0:511]\n",
    "            segments_tensor = segments_tensor[0:512]\n",
    "            label_ids = label_ids[0:511]\n",
    "            char_tokens_tensors = torch.cat( (char_tokens_tensors, sep_tensor),0)\n",
    "            word_tokens_tensors = torch.cat( (word_tokens_tensors, sep_tensor),0)\n",
    "            label_ids = torch.cat((label_ids,other_label),0)\n",
    "        return (name,char_tokens_tensors,word_tokens_tensors,segments_tensor,label_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def create_mini_batch(self,samples):\n",
    "        # sample[0]:name\n",
    "        # sample[1]:char\n",
    "        # sample[2]:word\n",
    "        # sample[3]:segments\n",
    "        # sample[4]:mask\n",
    "        # sample[5]:label\n",
    "        \n",
    "        name = [s[0] for s in samples]\n",
    "        char_tokens_tensors = [s[1] for s in samples]\n",
    "        word_tokens_tensors = [s[2] for s in samples]\n",
    "        segments_tensors = [s[3] for s in samples]\n",
    "        if self.train_or_test=='train':\n",
    "            label_tensors= [s[4] for s in samples]\n",
    "        else:\n",
    "            label_tensors=None\n",
    "        \n",
    "        before_pad_length=[]\n",
    "        for i in range(len(char_tokens_tensors)):\n",
    "            before_pad_length.append(char_tokens_tensors[i].shape[0])\n",
    "        \n",
    "        char_tokens_tensors = pad_sequence(char_tokens_tensors,batch_first=True)\n",
    "        word_tokens_tensors = pad_sequence(word_tokens_tensors,batch_first=True)\n",
    "        segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n",
    "        max_pad_length = char_tokens_tensors[0].shape[0]\n",
    "        masks_tensors = torch.zeros(char_tokens_tensors.shape,dtype=torch.long)\n",
    "        masks_tensors = masks_tensors.masked_fill(char_tokens_tensors != 0, 1)\n",
    "        if self.train_or_test=='train':\n",
    "            other_label = torch.tensor([1]*1+[0]*20,dtype=torch.float)\n",
    "            other_label = torch.unsqueeze(other_label,0)\n",
    "            for i in range(len(before_pad_length)):\n",
    "                for j in range(max_pad_length-before_pad_length[i]):\n",
    "                    label_tensors[i] = torch.cat((label_tensors[i],other_label),0)\n",
    "            r_label_tensors=torch.stack([i for i in (label_tensors)])\n",
    "        else:\n",
    "            r_label_tensors=None\n",
    "            \n",
    "        return name,char_tokens_tensors,word_tokens_tensors,segments_tensors,masks_tensors,r_label_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2228\n",
      "tensor([   2,  164, 1137,  240,  683,    3,    1,    1,  607,  544,    8,  393,\n",
      "         279,   46,   11,  141,    8,  623,  306,   24,  587,  477,    8,  218,\n",
      "          46,   11,  149, 1003,  183,  231,  210,   18,    6,  570,  573,   15,\n",
      "         790,    9,   46,   42,    6,   14,   46,   11,   10,    1,    1,    1,\n",
      "         576,  169,   91,  271,  233,   76,  484, 1164,  411,  779,  230,   28,\n",
      "          34,  352,  425, 1328,  214,  587,  777,  201,  464,    6,   58,  607,\n",
      "         544,  393,  279,  141,    6,  587,  477,    8,  218,   46,   11,  240,\n",
      "         514,   59,    6,   58,    1,  607,  544,    8,  393,  279,   46,   11,\n",
      "          44,   14,   20,   17,   74,   24,   19,  141,   59,    8, 1508,  191,\n",
      "          16,   24,   19,  141,   17,   36,   11,   44,   14,   10,    1,    1,\n",
      "           1,   80,  172,    6,  233,   76, 1588,   12,  694,   76,    8,   94,\n",
      "          19,   18,   12,    7,  730,  255,    1,    1,   34,    1,    1,   34,\n",
      "           1,    1,   25,  253,    1,  275,  147,    1,    1,   34,    1,    1,\n",
      "          34,    1,    1,   25,  253,    1,  607,  544,  393,  279,  587,  477,\n",
      "           1,  210,  716, 1025,  441,   95,  587,  477,    1,    6,   58,  440,\n",
      "         293,    6,  600,  651,  378,   59,   17,   58,    1,   59,  890,   16,\n",
      "          66,   12,   58,    1,   59,    6,  378,  568,    8,  477,  359,   79,\n",
      "          37,   23,   18,   19,   11,  141,   17,   36,   11,   44,   14,   10,\n",
      "        1588,   12,  191, 1508,  607,  544,  393,  279,  587,  477,   15,  351,\n",
      "          16,   18,   19,   24,   19,  141,   17,    7,  164, 1137,   48,   53,\n",
      "          17,    8,  607,  544,  393,  279,  141,  587,  477, 1076,  599,   15,\n",
      "         287,   79,    7,  191, 1508,  378,  568,    8,  477,  359,   79,   37,\n",
      "          23,    9,  141,   17,   36,   11,   44,   14,   10,    1,    1,    1,\n",
      "         174,  192,    7,   80, 1588,   12,  271,  483,  218,  613,  201,  218,\n",
      "         378,   32,   31,  721,  718,  352,  416,  359,    6,  904,  519, 1588,\n",
      "          12,  867,  364,    8,  613,   11,  335,  143,  904,  519,  378,    6,\n",
      "          91,  271,  847,  155,   15,  287,   79,   18,   19,   24,   19,   44,\n",
      "          14,   10,    1,    1,    1,   77,  240,  683,    8,  514,   16,    9,\n",
      "         475,  555,  119,   13,  140,   28,   15, 1185,  164,   17,   74,   11,\n",
      "          44,   14,   15,  815,  266,   46,   11,    9,   68,    7,  183,  231,\n",
      "           1,    1,    1,    6,  570,  573,   15,  790,    9,   16,    9,  314,\n",
      "         531,   15,  730,  255,    1,   25,    1,   57,    1,    1,   48,    1,\n",
      "         300,    1,    1,    1,  107,   53,   17,    8,  313,  393,    7, 1395,\n",
      "         282,    7,  280,  139,  703,    7,  345,  163,  104,   13,   29,  209,\n",
      "           6, 1660,  359,  378,    6,  179,  233,    8,   54,   39,  600,  102,\n",
      "          46,   11,   44,   14,   10,   24,   87,    7, 1508,  191,  587,  477,\n",
      "          15,  351,   16,   24,   19,  141,    8,    6,  176,  730,  255,    1,\n",
      "          25,    1,   57,    1,    1,   48,    1,  352,    1,    1,    1,  107,\n",
      "          53,   17,    8,    7,  190,  360,   15,   91,   51,   10,    1,  960,\n",
      "         114,  815,  266,  314,    1,  164, 1137,  462,  266,  108,   99,    8,\n",
      "         419,  269,   15,  476,  359,    1,    1,  583,  164,  384,  229,  314,\n",
      "           1,  867,  364, 1097,  294,   99,   32,   31,  119,   13,  140,   28,\n",
      "         362,  122,  159,  365,   53,   17,    6,    3])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = NERset('train')\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=False,collate_fn=dataset.create_mini_batch)\n",
    "    print(len(dataloader))\n",
    "    for i,data in enumerate(dataloader):\n",
    "        if data[1].shape[1]>=512:\n",
    "            print(data[1][1])\n",
    "            print(data[3][1])\n",
    "            #print(data[5])\n",
    "            break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
